#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Tue Aug  1 10:39:20 2017@author: samuel"""import scrapyfrom allfund.items import AllfundItemfrom bs4 import BeautifulSoupimport reclass AllfundSpider(scrapy.Spider):    name = 'allfund'    allowed_domains = ['eastmoney.com']    start_urls = ['http://fund.eastmoney.com/allfund.html#0']    '''#获取首页内容，内容传给parse()    def start_requests(self):        reqs = scrapy.Request('http://fund.eastmoney.com/allfund.html#0')            print ('start_requests  req======',reqs)        return reqs'''    #解析首页内容，将解析出的基金代码页面 传给 parse_detail()    def parse(self,response):        #print('   response ======= ',response)        import chardet        print(chardet.detect(response.body))        soup = BeautifulSoup(response.body,'html.parser',from_encoding='GB2312')        #print('   soup ======= ',soup)        uls = soup.find_all('ul',class_='num_right')        for ul in uls:            for each in ul.find_all('li'):                li_list = each.find_all('a')                fund_info_dict = {'fund_id':'','fund_name':''}                if len(li_list)>1:                    fund=li_list[0].text                    fund_id=re.findall(r'\d+',fund)[0]                    fund_url=li_list[0].attrs['href']                    #fund_name=fund.decode('utf-8')[fund.find(ur'）')+1:].encode('utf8')                    fund_name=li_list[0].text[8:]                    #print('fund_name ==',fund_name,fund_id)                    fund_info_dict['fund_id'] = fund_id                    fund_info_dict['fund_name'] = fund_name                    #fund_info_dict['fund_url'] = fund_url                    #funds_text.append(fund_info_dict)                    newurl = 'http://fund.eastmoney.com/%s.html'%fund_id                    #print('newurl =========',newurl)                    yield scrapy.Request(newurl, meta=fund_info_dict, callback=self.parse_detail)    #解析每个基金的页面，获取每个阶段的业绩    def parse_detail(self,response):        fund_data = AllfundItem()        fund_data['fund_id'] = response.meta['fund_id']        fund_data['fund_name'] = response.meta['fund_name']        #print('fund_data1 ========',fund_data)        if response is None:            #print('html_cont ====== none')            return        soup = BeautifulSoup(response.body,'html.parser',from_encoding='gb2312')        #find div        data_of_fund = soup.find_all('div', class_="dataOfFund")        if data_of_fund:            #get the last div            data_itmes = data_of_fund[-1].find_all('dd',class_=None)            if not data_itmes:                print ('!!{} No data find'.format(soup.title))            else:                fund_data['one_month'] = data_itmes[0].find_all('span')[1].text                fund_data['three_month'] = data_itmes[1].find_all('span')[1].text                fund_data['six_month'] = data_itmes[2].find_all('span')[1].text                fund_data['one_year'] = data_itmes[3].find_all('span')[1].text                fund_data['three_year'] = data_itmes[4].find_all('span')[1].text                fund_data['from_start'] = data_itmes[5].find_all('span')[1].text            #print('into mysql, fund_data ======== ',fund_data)            yield fund_data